{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dbt + Spark on Dataproc → BigQuery Demo\n",
    "\n",
    "This notebook demonstrates an end-to-end workflow:\n",
    "1. Create a Dataproc cluster with Spark Thrift server\n",
    "2. Use dbt-spark to run transformations on the cluster\n",
    "3. Write final results to BigQuery using Spark BigQuery connector\n",
    "\n",
    "## Architecture\n",
    "```\n",
    "Local dbt → Dataproc Spark (Thrift) → Hive Tables (GCS) → BigQuery Tables\n",
    "```\n",
    "\n",
    "## Prerequisites\n",
    "- GCP project with billing enabled\n",
    "- gcloud CLI authenticated\n",
    "- Python 3.8+\n",
    "- jaffle-shop-classic repository at `/Users/[username]/Developer/git/jaffle-shop-classic`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# GCP Configuration\n",
    "PROJECT_ID = \"johanesa-playground-326616\"\n",
    "REGION = \"us-central1\"\n",
    "ZONE = f\"{REGION}-a\"\n",
    "BQ_LOCATION = \"US\"\n",
    "\n",
    "# Resource Names\n",
    "CLUSTER_NAME = \"dbt-spark-demo-cluster\"\n",
    "GCS_BUCKET = \"johanesa-dbt-spark-demo\"\n",
    "BQ_DATASET = \"jaffle_shop_demo\"\n",
    "HIVE_WAREHOUSE = f\"gs://{GCS_BUCKET}/hive-warehouse\"\n",
    "\n",
    "# Paths\n",
    "JAFFLE_SHOP_PATH = Path(\"/Users/[username]/Developer/git/jaffle-shop-classic\")\n",
    "DBT_PROFILES_DIR = Path.home() / \".dbt\"\n",
    "\n",
    "print(f\"✓ Project ID: {PROJECT_ID}\")\n",
    "print(f\"✓ Region: {REGION}\")\n",
    "print(f\"✓ Cluster: {CLUSTER_NAME}\")\n",
    "print(f\"✓ GCS Bucket: {GCS_BUCKET}\")\n",
    "print(f\"✓ BQ Dataset: {BQ_DATASET} (location: {BQ_LOCATION})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_command(cmd, check=True, capture_output=True):\n",
    "    \"\"\"Run shell command and return result\"\"\"\n",
    "    print(f\"Running: {cmd}\")\n",
    "    result = subprocess.run(\n",
    "        cmd,\n",
    "        shell=True,\n",
    "        check=False,  # Don't raise exception immediately\n",
    "        capture_output=capture_output,\n",
    "        text=True\n",
    "    )\n",
    "\n",
    "    # Always show stdout\n",
    "    if result.stdout:\n",
    "        print(result.stdout)\n",
    "\n",
    "    # Always show stderr if present\n",
    "    if result.stderr:\n",
    "        if result.returncode != 0:\n",
    "            print(f\"ERROR: {result.stderr}\")\n",
    "        else:\n",
    "            print(f\"Warning: {result.stderr}\")\n",
    "\n",
    "    # Raise exception if check=True and command failed\n",
    "    if check and result.returncode != 0:\n",
    "        raise subprocess.CalledProcessError(\n",
    "            result.returncode,\n",
    "            cmd,\n",
    "            output=result.stdout,\n",
    "            stderr=result.stderr\n",
    "        )\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def wait_for_cluster(cluster_name, project_id, region, timeout=600):\n",
    "    \"\"\"Wait for cluster to be ready, handling ERROR state\"\"\"\n",
    "    print(f\"Waiting for cluster {cluster_name} to be ready...\")\n",
    "    start_time = time.time()\n",
    "    while time.time() - start_time < timeout:\n",
    "        result = run_command(\n",
    "            f\"gcloud dataproc clusters describe {cluster_name} --project={project_id} --region={region} --format=json\", check=False)\n",
    "        if result.returncode == 0:\n",
    "            cluster_info = json.loads(result.stdout)\n",
    "            status = cluster_info.get('status', {}).get('state')\n",
    "            print(f\"Cluster status: {status}\")\n",
    "\n",
    "            if status == 'RUNNING':\n",
    "                print(\"✓ Cluster is ready!\")\n",
    "                return cluster_info\n",
    "            elif status == 'ERROR':\n",
    "                error_details = cluster_info.get(\n",
    "                    'status', {}).get('details', 'Unknown error')\n",
    "                raise RuntimeError(\n",
    "                    f\"Cluster creation failed! Status: ERROR. Details: {error_details}\")\n",
    "\n",
    "        time.sleep(10)\n",
    "\n",
    "    raise TimeoutError(\n",
    "        f\"Cluster did not become ready within {timeout} seconds\")\n",
    "\n",
    "\n",
    "def get_cluster_master_ip(cluster_name, project_id, region):\n",
    "    result = run_command(\n",
    "        f\"gcloud dataproc clusters describe {cluster_name} --project={project_id} --region={region} --format='value(config.masterConfig.instanceNames[0])'\")\n",
    "    master_instance = result.stdout.strip()\n",
    "    result = run_command(\n",
    "        f\"gcloud compute instances describe {master_instance} --project={project_id} --zone={ZONE} --format='value(networkInterfaces[0].networkIP)'\")\n",
    "    return result.stdout.strip()\n",
    "\n",
    "\n",
    "print(\"✓ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Verify Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "result = run_command(\"gcloud --version\")\n",
    "print(\"✓ gcloud CLI is installed\\n\")\n",
    "\n",
    "result = run_command(\"gcloud config get-value project\")\n",
    "current_project = result.stdout.strip()\n",
    "print(f\"Current project: {current_project}\")\n",
    "\n",
    "if current_project != PROJECT_ID:\n",
    "    print(f\"Setting project to {PROJECT_ID}...\")\n",
    "    run_command(f\"gcloud config set project {PROJECT_ID}\")\n",
    "\n",
    "if not JAFFLE_SHOP_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"jaffle-shop-classic not found at {JAFFLE_SHOP_PATH}\")\n",
    "print(f\"✓ jaffle-shop-classic found at {JAFFLE_SHOP_PATH}\")\n",
    "\n",
    "print(f\"✓ Python version: {sys.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create GCS Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_command(f\"gsutil ls -b gs://{GCS_BUCKET}\", check=False)\n",
    "\n",
    "if result.returncode != 0:\n",
    "    print(f\"Creating GCS bucket: {GCS_BUCKET}\")\n",
    "    run_command(f\"gsutil mb -p {PROJECT_ID} -l {REGION} gs://{GCS_BUCKET}\")\n",
    "    print(f\"✓ Bucket created: gs://{GCS_BUCKET}\")\n",
    "else:\n",
    "    print(f\"✓ Bucket already exists: gs://{GCS_BUCKET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Dataproc Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_command(\n",
    "    f\"gcloud dataproc clusters describe {CLUSTER_NAME} --project={PROJECT_ID} --region={REGION}\", check=False)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(f\"⚠ Cluster {CLUSTER_NAME} already exists\")\n",
    "else:\n",
    "    print(f\"Creating Dataproc cluster: {CLUSTER_NAME}\")\n",
    "    print(\"This may take 3-5 minutes...\\n\")\n",
    "    create_cmd = f\"gcloud dataproc clusters create {CLUSTER_NAME} --project={PROJECT_ID} --region={REGION} --zone={ZONE} --single-node --master-machine-type=n1-standard-4 --master-boot-disk-size=100GB --image-version=2.1-debian11 --properties=hive:hive.metastore.warehouse.dir={HIVE_WAREHOUSE} --enable-component-gateway --optional-components=JUPYTER\"\n",
    "    run_command(create_cmd)\n",
    "    print(\"\\n✓ Cluster creation initiated\")\n",
    "\n",
    "cluster_info = wait_for_cluster(CLUSTER_NAME, PROJECT_ID, REGION)\n",
    "print(\"\\n✓ Cluster is ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Start Spark Thrift Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting Spark Thrift Server...\")\n",
    "start_cmd = f\"gcloud compute ssh {CLUSTER_NAME}-m --project={PROJECT_ID} --zone={ZONE} --command='sudo /usr/lib/spark/sbin/start-thriftserver.sh --master yarn --deploy-mode client --conf spark.sql.warehouse.dir={HIVE_WAREHOUSE} --conf spark.hadoop.hive.metastore.warehouse.dir={HIVE_WAREHOUSE}'\"\n",
    "run_command(start_cmd, check=False)\n",
    "\n",
    "print(\"Waiting for Thrift server to start...\")\n",
    "time.sleep(15)\n",
    "\n",
    "verify_cmd = f\"gcloud compute ssh {CLUSTER_NAME}-m --project={PROJECT_ID} --zone={ZONE} --command='netstat -tuln | grep 10001'\"\n",
    "result = run_command(verify_cmd, check=False)\n",
    "if result.returncode == 0:\n",
    "    print(\"✓ Thrift server is running on port 10001\")\n",
    "\n",
    "master_ip = get_cluster_master_ip(CLUSTER_NAME, PROJECT_ID, REGION)\n",
    "print(f\"\\n✓ Cluster master IP: {master_ip}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Install dbt-spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Installing dbt-spark...\\n\")\n",
    "run_command(\"pip install -q dbt-core dbt-spark[PyHive]\")\n",
    "result = run_command(\"dbt --version\")\n",
    "print(\"\\n✓ dbt-spark installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Configure dbt Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import getpass\n",
    "\n",
    "DBT_PROFILES_DIR.mkdir(exist_ok=True)\n",
    "username = getpass.getuser()\n",
    "\n",
    "profiles_config = {'jaffle_shop': {'target': 'dev', 'outputs': {'dev': {'type': 'spark', 'method': 'thrift',\n",
    "                                                                        'schema': 'default', 'host': master_ip, 'port': 10001, 'user': username, 'connect_timeout': 60, 'connect_retries': 3}}}}\n",
    "\n",
    "profiles_path = DBT_PROFILES_DIR / 'profiles.yml'\n",
    "existing_profiles = {}\n",
    "if profiles_path.exists():\n",
    "    with open(profiles_path, 'r') as f:\n",
    "        existing_profiles = yaml.safe_load(f) or {}\n",
    "\n",
    "existing_profiles.update(profiles_config)\n",
    "with open(profiles_path, 'w') as f:\n",
    "    yaml.dump(existing_profiles, f, default_flow_style=False)\n",
    "\n",
    "print(f\"✓ dbt profile created at {profiles_path}\")\n",
    "print(f\"  Host: {master_ip}, Port: 10001, Schema: default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test dbt Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing dbt connection...\\n\")\n",
    "os.chdir(JAFFLE_SHOP_PATH)\n",
    "result = run_command(\"dbt debug\", check=False)\n",
    "if result.returncode == 0:\n",
    "    print(\"\\n✓ dbt connection successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Run dbt Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running dbt seed...\\n\")\n",
    "os.chdir(JAFFLE_SHOP_PATH)\n",
    "run_command(\"dbt seed\")\n",
    "print(\"\\n✓ Seed data loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Run dbt Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running dbt transformations...\\n\")\n",
    "os.chdir(JAFFLE_SHOP_PATH)\n",
    "run_command(\"dbt run\")\n",
    "print(\"\\n✓ Transformations complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Create BigQuery Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Creating BigQuery dataset: {BQ_DATASET}\\n\")\n",
    "result = run_command(\n",
    "    f\"bq ls -d --project_id={PROJECT_ID} {BQ_DATASET}\", check=False)\n",
    "if result.returncode != 0:\n",
    "    run_command(\n",
    "        f\"bq mk --project_id={PROJECT_ID} --location={BQ_LOCATION} {BQ_DATASET}\")\n",
    "    print(f\"✓ Dataset created: {PROJECT_ID}.{BQ_DATASET}\")\n",
    "else:\n",
    "    print(f\"✓ Dataset exists: {PROJECT_ID}.{BQ_DATASET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Write to BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark_script = f'''from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Write to BigQuery\").config(\"spark.sql.warehouse.dir\", \"{HIVE_WAREHOUSE}\").enableHiveSupport().getOrCreate()\n",
    "tables = [\"customers\", \"orders\"]\n",
    "for table in tables:\n",
    "    print(f\"Writing {{table}} to BigQuery...\")\n",
    "    df = spark.table(f\"default.{{table}}\")\n",
    "    df.show(5)\n",
    "    df.write.format(\"bigquery\").option(\"table\", f\"{PROJECT_ID}.{BQ_DATASET}.{{table}}\").option(\"temporaryGcsBucket\", \"{GCS_BUCKET}\").mode(\"overwrite\").save()\n",
    "    print(f\"✓ {{table}} written\")\n",
    "print(\"\\n✓ All tables written!\")\n",
    "spark.stop()'''\n",
    "\n",
    "script_path = \"/tmp/write_to_bq.py\"\n",
    "with open(script_path, 'w') as f:\n",
    "    f.write(pyspark_script)\n",
    "\n",
    "run_command(\n",
    "    f\"gcloud compute scp {script_path} {CLUSTER_NAME}-m:/tmp/write_to_bq.py --project={PROJECT_ID} --zone={ZONE}\")\n",
    "print(\"\\nSubmitting PySpark job...\\n\")\n",
    "run_command(\n",
    "    f\"gcloud dataproc jobs submit pyspark /tmp/write_to_bq.py --cluster={CLUSTER_NAME} --project={PROJECT_ID} --region={REGION}\")\n",
    "print(\"\\n✓ Tables written to BigQuery!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Verify Data in BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Verifying data in BigQuery...\\n\")\n",
    "print(\"=\" * 60)\n",
    "print(\"CUSTOMERS TABLE\")\n",
    "print(\"=\" * 60)\n",
    "run_command(\n",
    "    f\"bq query --project_id={PROJECT_ID} --use_legacy_sql=false 'SELECT * FROM {BQ_DATASET}.customers LIMIT 5'\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ORDERS TABLE\")\n",
    "print(\"=\" * 60)\n",
    "run_command(\n",
    "    f\"bq query --project_id={PROJECT_ID} --use_legacy_sql=false 'SELECT * FROM {BQ_DATASET}.orders LIMIT 5'\")\n",
    "\n",
    "print(\"\\n✓ Data verification complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Cleanup (Optional)\n",
    "\n",
    "Run this cell to delete the Dataproc cluster and save costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Deleting Dataproc cluster...\")\n",
    "run_command(\n",
    "    f\"gcloud dataproc clusters delete {CLUSTER_NAME} --project={PROJECT_ID} --region={REGION} --quiet\")\n",
    "print(f\"\\n✓ Cluster {CLUSTER_NAME} deleted\")\n",
    "print(\n",
    "    f\"\\nNote: GCS bucket ({GCS_BUCKET}) and BigQuery dataset ({BQ_DATASET}) were NOT deleted.\")\n",
    "print(\"Delete them manually if needed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. ✓ Created Dataproc cluster with Spark Thrift server\n",
    "2. ✓ Configured dbt-spark to connect via Thrift\n",
    "3. ✓ Ran dbt transformations on Spark\n",
    "4. ✓ Wrote results to BigQuery using Spark BigQuery connector\n",
    "\n",
    "### Resources Created\n",
    "- Dataproc Cluster: `dbt-spark-demo-cluster`\n",
    "- GCS Bucket: `johanesa-dbt-spark-demo`\n",
    "- BigQuery Dataset: `jaffle_shop_demo`\n",
    "- BigQuery Tables: `customers`, `orders`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
