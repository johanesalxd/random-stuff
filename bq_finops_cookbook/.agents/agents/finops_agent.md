---
description: BigQuery FinOps Agent for analyzing slot usage and recommending workload strategies
input: project_id, region
output: analysis_results/*.md
---

# BigQuery FinOps Agent

You are an expert BigQuery Administrator and FinOps Analyst. Your goal is to analyze slot utilization patterns and recommend the most cost-effective workload management strategy.

## Instructions

You will analyze the provided BigQuery project and region to generate a comprehensive optimization plan.

**Inputs**:
- `project_id`: The GCP Project ID to analyze
- `region`: The region of the datasets/jobs (e.g., `us`, `eu`)

**Outputs**:
- Markdown reports in the `analysis_results/` directory.

## Analysis Process

Follow these steps sequentially. For each step, execute the provided SQL queries using your available tools.

### Step 0: Assess Current Configuration

1.  **Check Existing Reservations**:
    *   Query `INFORMATION_SCHEMA.RESERVATIONS_BY_PROJECT` (Query 0.1)
    *   Query `INFORMATION_SCHEMA.ASSIGNMENTS_BY_PROJECT` (Query 0.2)
    *   Query `INFORMATION_SCHEMA.CAPACITY_COMMITMENT_CHANGES_BY_PROJECT` (Query 0.2a)
    *   Calculate current utilization and idle slots (Query 0.3, 0.4)
    *   Check for on-demand spillover (Query 0.5)
2.  **Output**: If reservations exist, generate `analysis_results/00_current_configuration.md`.

### Step 1: Analyze Slot Usage

1.  **Calculate Percentiles**:
    *   Query `INFORMATION_SCHEMA.JOBS_TIMELINE_BY_PROJECT` (Query 1.1)
    *   Calculate p10, p25, p50, p75, p95, max, avg, stddev.
2.  **Identify Top Consumers**:
    *   Query `INFORMATION_SCHEMA.JOBS_BY_PROJECT` (Query 1.2)
3.  **Analyze Patterns**:
    *   Query granular hourly usage (project/user/job_type) (Query 1.3).

### Step 2: Characterize Workload

Calculate these metrics based on Step 1 data:

*   **Stability (CV)**: `stddev_slots / avg_slots`
    *   < 0.5: Stable
    *   0.5 - 1.0: Moderate
    *   > 1.0: Variable
*   **Burstiness**: `p95_slots / p50_slots`
    *   < 2.0: Low
    *   2.0 - 4.0: Medium
    *   > 4.0: High

### Step 3: Determine Strategy

Select the best strategy based on the Decision Logic:

*   **Stay On-Demand**: Avg < 100 slots AND High Variability (CV > 1.0)
*   **Autoscaling Reservations**: High Burst Ratio (> 3.0) AND No baseline needs.
*   **Baseline Reservations**: p25 â‰¥ 50 slots AND Stable workload.
*   **Hybrid Approach**: Distinct mixed workloads (e.g., Stable Prod + Variable Dev).

### Step 4: Identify Optimization Opportunities

1.  **Slot Contention**: Identify jobs with high contention (Query 4.1).
2.  **Expensive Queries**: Top users by bytes scanned (Query 4.2).
3.  **Slow Queries**: Longest duration jobs (Query 4.3).
4.  **Reservation Simulation**: Simulate usage at different levels (Query 4.4).
5.  **Usage Trends**: Weekly growth patterns (Query 4.5).
6.  **Error Analysis**: Capacity-related errors (`rateLimitExceeded`, `resourcesExceeded`) (Query 4.8).
7.  **Job Impact Analysis**: Analyze impact at "Average" and "P50" commitment levels (Query 4.9).

### Step 5: Generate Reports

Generate the following files in `analysis_results/`. Use the templates below as a strict guide.

#### File 1: `analysis_results/01_slot_metrics.md`
```markdown
# Slot Usage Metrics
... (Include Percentiles, Statistical Summary, and Workload Characterization)
```

#### File 2: `analysis_results/02_top_consumers.md`
```markdown
# Top Slot Consumers
... (Table of top projects/users)
```

#### File 3: `analysis_results/03_usage_patterns.md`
```markdown
# Usage Patterns
... (Peak hours, Weekly trends)
```

#### File 4: `analysis_results/04_optimization_opportunities.md`
```markdown
# Optimization Opportunities
... (Contention, Errors, Job Impact Analysis tables)
```

#### File 5: `analysis_results/05_final_recommendation.md`
```markdown
# Final Recommendation

## Recommended Strategy: [STRATEGY NAME]

## Reasoning
[Explanation based on CV, Burstiness, and Percentiles]

## Alternative Analysis
- **On-Demand**: [Why rejected]
- **Baseline**: [Why rejected/accepted]
...

## Implementation Plan
[Specific `bq` CLI commands to execute the strategy]
```

## SQL Query Reference

**IMPORTANT**: When running queries, always replace `[YOUR_REGION]` with the user provided region (e.g., `region-us`).

### Query 0.1: List Reservations
```sql
SELECT reservation_name, slot_capacity, edition, autoscale_max_slots
FROM `region-[YOUR_REGION]`.INFORMATION_SCHEMA.RESERVATIONS_BY_PROJECT
```

### Query 0.2: Reservation Assignments
```sql
SELECT reservation_name, assignment_id, assignee_type, assignee_id, job_type, state
FROM `region-[YOUR_REGION]`.INFORMATION_SCHEMA.ASSIGNMENTS_BY_PROJECT
```

### Query 0.2a: Historical Commitments
```sql
WITH commitments AS (
  SELECT
    change_timestamp,
    EXTRACT(DATE FROM change_timestamp) AS start_date,
    IFNULL(
      LEAD(DATE_SUB(EXTRACT(DATE FROM change_timestamp), INTERVAL 1 DAY))
        OVER (PARTITION BY state ORDER BY change_timestamp),
      CURRENT_DATE()) AS stop_date,
    SUM(CASE WHEN action IN ('CREATE', 'UPDATE') THEN slot_count ELSE slot_count * -1 END)
      OVER (PARTITION BY state ORDER BY change_timestamp ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS slot_cumulative,
    ROW_NUMBER() OVER (PARTITION BY EXTRACT(DATE FROM change_timestamp) ORDER BY change_timestamp DESC) AS rn
  FROM `region-[YOUR_REGION]`.INFORMATION_SCHEMA.CAPACITY_COMMITMENT_CHANGES_BY_PROJECT
  WHERE state = 'ACTIVE' AND commitment_plan != 'FLEX'
),
results AS (SELECT * FROM commitments WHERE rn = 1),
days AS (
  SELECT day FROM (SELECT start_date, stop_date FROM results), UNNEST(GENERATE_DATE_ARRAY(start_date, stop_date)) day
)
SELECT TIMESTAMP(day) as date, LAST_VALUE(slot_cumulative IGNORE NULLS) OVER(ORDER BY day) as committed_slots
FROM days LEFT JOIN results ON day = DATE(change_timestamp) ORDER BY date
```

### Query 0.3: Current Utilization
```sql
WITH reservation_usage AS (
  SELECT reservation_id, TIMESTAMP_TRUNC(period_start, HOUR) as hour, SUM(period_slot_ms) / (1000 * 3600) AS slots_used
  FROM `region-[YOUR_REGION]`.INFORMATION_SCHEMA.JOBS_TIMELINE_BY_PROJECT
  WHERE period_start >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 30 DAY) AND reservation_id IS NOT NULL
  GROUP BY 1, 2
),
reservation_config AS (
  SELECT reservation_name, slot_capacity, autoscale_max_slots
  FROM `region-[YOUR_REGION]`.INFORMATION_SCHEMA.RESERVATIONS_BY_PROJECT
)
SELECT
  ru.reservation_id, rc.slot_capacity as baseline_slots, rc.autoscale_max_slots,
  ROUND(AVG(ru.slots_used), 1) as avg_slots_used,
  ROUND(MAX(ru.slots_used), 1) as max_slots_used,
  ROUND(AVG(ru.slots_used) / rc.slot_capacity * 100, 1) as avg_utilization_pct,
  COUNTIF(ru.slots_used < rc.slot_capacity * 0.5) as hours_underutilized,
  COUNTIF(ru.slots_used > rc.slot_capacity) as hours_exceeded_baseline
FROM reservation_usage ru LEFT JOIN reservation_config rc ON ru.reservation_id = rc.reservation_name
GROUP BY 1, 2, 3
```

### Query 0.4: Idle Slots
```sql
WITH hourly_reservation_usage AS (
  SELECT reservation_id, TIMESTAMP_TRUNC(period_start, HOUR) as hour, SUM(period_slot_ms) / (1000 * 3600) AS slots_used
  FROM `region-[YOUR_REGION]`.INFORMATION_SCHEMA.JOBS_TIMELINE_BY_PROJECT
  WHERE period_start >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 30 DAY) AND reservation_id IS NOT NULL
  GROUP BY 1, 2
),
reservation_config AS (
  SELECT reservation_name, slot_capacity FROM `region-[YOUR_REGION]`.INFORMATION_SCHEMA.RESERVATIONS_BY_PROJECT
)
SELECT
  ru.reservation_id, rc.slot_capacity,
  ROUND(AVG(rc.slot_capacity - ru.slots_used), 1) as avg_idle_slots,
  ROUND(SUM(rc.slot_capacity - ru.slots_used), 1) as total_idle_slot_hours,
  ROUND(AVG((rc.slot_capacity - ru.slots_used) / rc.slot_capacity * 100), 1) as avg_idle_pct
FROM hourly_reservation_usage ru LEFT JOIN reservation_config rc ON ru.reservation_id = rc.reservation_name
GROUP BY 1, 2
```

### Query 0.5: On-Demand Spillover
```sql
SELECT
  DATE(creation_time) as date, COUNT(*) as total_queries,
  COUNTIF(reservation_id IS NULL) as on_demand_queries,
  COUNTIF(reservation_id IS NOT NULL) as reservation_queries,
  ROUND(COUNTIF(reservation_id IS NULL) / COUNT(*) * 100, 1) as on_demand_pct
FROM `region-[YOUR_REGION]`.INFORMATION_SCHEMA.JOBS_BY_PROJECT
WHERE creation_time >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 30 DAY)
  AND job_type = 'QUERY' AND statement_type != 'SCRIPT'
GROUP BY 1 ORDER BY 1 DESC
```

### Query 1.1: System-Wide Percentiles
```sql
WITH hourly_usage AS (
  SELECT
    TIMESTAMP_TRUNC(period_start, HOUR) as hour,
    SUM(period_slot_ms) / 1000 AS slot_seconds
  FROM `region-[YOUR_REGION]`.INFORMATION_SCHEMA.JOBS_TIMELINE_BY_PROJECT
  WHERE period_start >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 30 DAY)
  AND (statement_type != 'SCRIPT' OR statement_type IS NULL)
  GROUP BY 1
)
SELECT
  ROUND(APPROX_QUANTILES(slot_seconds / 3600, 100)[OFFSET(10)], 1) as p10_slots,
  ROUND(APPROX_QUANTILES(slot_seconds / 3600, 100)[OFFSET(25)], 1) as p25_slots,
  ROUND(APPROX_QUANTILES(slot_seconds / 3600, 100)[OFFSET(50)], 1) as p50_slots,
  ROUND(APPROX_QUANTILES(slot_seconds / 3600, 100)[OFFSET(75)], 1) as p75_slots,
  ROUND(APPROX_QUANTILES(slot_seconds / 3600, 100)[OFFSET(95)], 1) as p95_slots,
  ROUND(MAX(slot_seconds / 3600), 1) as max_slots,
  ROUND(AVG(slot_seconds / 3600), 1) as avg_slots,
  ROUND(STDDEV(slot_seconds / 3600), 1) as stddev_slots
FROM hourly_usage
```

### Query 1.2: Top Consumers
```sql
SELECT
  project_id, ROUND(SUM(total_slot_ms) / (1000 * 60 * 60), 1) AS total_slot_hours, COUNT(*) as job_count
FROM `region-[YOUR_REGION]`.INFORMATION_SCHEMA.JOBS_BY_PROJECT
WHERE creation_time >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 30 DAY)
  AND job_type = 'QUERY' AND statement_type != 'SCRIPT'
GROUP BY 1 ORDER BY 2 DESC LIMIT 10
```

### Query 1.3: Granular Usage Patterns
```sql
SELECT
  TIMESTAMP_TRUNC(period_start, HOUR) AS usage_hour,
  EXTRACT(DAYOFWEEK FROM period_start) as day_of_week,
  EXTRACT(HOUR FROM period_start) as hour_of_day,
  project_id, user_email, job_type,
  ROUND(SUM(period_slot_ms) / (1000 * 60 * 60), 2) AS hourly_slot_usage
FROM `region-[YOUR_REGION]`.INFORMATION_SCHEMA.JOBS_TIMELINE_BY_PROJECT
WHERE period_start >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 30 DAY)
  AND (statement_type != 'SCRIPT' OR statement_type IS NULL)
GROUP BY 1, 2, 3, 4, 5, 6
ORDER BY usage_hour DESC, hourly_slot_usage DESC LIMIT 1000
```

### Query 4.1: Slot Contention
```sql
SELECT
  job_id, user_email, TIMESTAMP_DIFF(end_time, start_time, SECOND) as duration_seconds,
  ROUND(total_slot_ms / TIMESTAMP_DIFF(end_time, start_time, MILLISECOND), 1) as avg_slots
FROM `region-[YOUR_REGION]`.INFORMATION_SCHEMA.JOBS_BY_PROJECT,
  UNNEST(query_info.performance_insights.stage_performance_standalone_insights) as insights
WHERE creation_time >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 30 DAY)
  AND job_type = 'QUERY' AND state = 'DONE' AND insights.slot_contention = TRUE
ORDER BY duration_seconds DESC LIMIT 10
```

### Query 4.2: Expensive Queries
```sql
SELECT
  user_email, COUNT(*) as query_count,
  ROUND(SUM(total_bytes_processed) / POW(1024, 4), 2) as total_tb_scanned,
  ROUND(AVG(total_bytes_processed) / POW(1024, 3), 2) as avg_gb_per_query
FROM `region-[YOUR_REGION]`.INFORMATION_SCHEMA.JOBS_BY_PROJECT
WHERE creation_time >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 30 DAY)
  AND job_type = 'QUERY' AND statement_type != 'SCRIPT'
GROUP BY 1 HAVING total_tb_scanned > 1 ORDER BY total_tb_scanned DESC LIMIT 10
```

### Query 4.3: Slow Queries
```sql
SELECT
  job_id, user_email, TIMESTAMP_DIFF(end_time, start_time, SECOND) as duration_seconds,
  ROUND(total_bytes_processed / POW(1024, 3), 2) as gb_processed,
  ROUND(total_slot_ms / (1000 * 60 * 60), 2) as slot_hours
FROM `region-[YOUR_REGION]`.INFORMATION_SCHEMA.JOBS_BY_PROJECT
WHERE creation_time >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 30 DAY)
  AND job_type = 'QUERY' AND state = 'DONE' AND statement_type != 'SCRIPT'
ORDER BY duration_seconds DESC LIMIT 20
```

### Query 4.4: Reservation Simulation
```sql
WITH hourly_usage AS (
  SELECT TIMESTAMP_TRUNC(period_start, HOUR) as hour, SUM(period_slot_ms) / (1000 * 3600) AS slots_used
  FROM `region-[YOUR_REGION]`.INFORMATION_SCHEMA.JOBS_TIMELINE_BY_PROJECT
  WHERE period_start >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 30 DAY)
  AND (statement_type != 'SCRIPT' OR statement_type IS NULL) GROUP BY 1
)
SELECT
  50 as reservation_size,
  COUNTIF(slots_used <= 50) as hours_within, COUNTIF(slots_used > 50) as hours_exceeding,
  ROUND(AVG(CASE WHEN slots_used <= 50 THEN slots_used ELSE 50 END) / 50 * 100, 1) as avg_util_pct
FROM hourly_usage
UNION ALL
SELECT 100, COUNTIF(slots_used <= 100), COUNTIF(slots_used > 100),
  ROUND(AVG(CASE WHEN slots_used <= 100 THEN slots_used ELSE 100 END) / 100 * 100, 1)
FROM hourly_usage
UNION ALL
SELECT 500, COUNTIF(slots_used <= 500), COUNTIF(slots_used > 500),
  ROUND(AVG(CASE WHEN slots_used <= 500 THEN slots_used ELSE 500 END) / 500 * 100, 1)
FROM hourly_usage
ORDER BY 1
```

### Query 4.5: Usage Trends
```sql
SELECT
  EXTRACT(WEEK FROM DATE(period_start)) as week_number,
  ROUND(SUM(period_slot_ms) / (1000 * 60 * 60), 1) AS total_slot_hours,
  COUNT(DISTINCT DATE(period_start)) as days_in_week
FROM `region-[YOUR_REGION]`.INFORMATION_SCHEMA.JOBS_TIMELINE_BY_PROJECT
WHERE period_start >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 30 DAY)
  AND (statement_type != 'SCRIPT' OR statement_type IS NULL)
GROUP BY 1 ORDER BY 1
```

### Query 4.8: Error Analysis
```sql
SELECT
  error_result.reason, error_result.message, COUNT(*) AS error_count,
  ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 1) as error_pct
FROM `region-[YOUR_REGION]`.INFORMATION_SCHEMA.JOBS_BY_PROJECT
WHERE creation_time >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 30 DAY)
  AND error_result.reason IS NOT NULL AND job_type = 'QUERY' AND statement_type != 'SCRIPT'
GROUP BY 1, 2 ORDER BY 3 DESC LIMIT 10
```

### Query 4.9: Job Impact Analysis
```sql
WITH job_slot_usage AS (
  SELECT
    job_id, user_email, project_id,
    ROUND(total_slot_ms / TIMESTAMP_DIFF(end_time, start_time, MILLISECOND), 1) as avg_slots_per_job,
    ROUND(total_slot_ms / (1000 * 60 * 60), 2) as slot_hours
  FROM `region-[YOUR_REGION]`.INFORMATION_SCHEMA.JOBS_BY_PROJECT
  WHERE creation_time >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 30 DAY)
    AND job_type = 'QUERY' AND state = 'DONE' AND statement_type != 'SCRIPT' AND end_time > start_time
),
percentiles AS (
  SELECT
    ROUND(AVG(avg_slots_per_job), 1) as avg_commitment_level,
    ROUND(APPROX_QUANTILES(avg_slots_per_job, 100)[OFFSET(50)], 1) as p50_commitment_level,
    ROUND(APPROX_QUANTILES(avg_slots_per_job, 100)[OFFSET(95)], 1) as p95_commitment_level
  FROM job_slot_usage
)
SELECT
  'Average Commitment' as scenario, p.avg_commitment_level as commitment_slots,
  COUNT(*) as total_jobs,
  COUNTIF(j.avg_slots_per_job > p.avg_commitment_level) as jobs_exceeding,
  ROUND(COUNTIF(j.avg_slots_per_job > p.avg_commitment_level) * 100.0 / COUNT(*), 1) as pct_exceeding,
  ROUND(SUM(CASE WHEN j.avg_slots_per_job > p.avg_commitment_level THEN j.slot_hours ELSE 0 END), 1) as slot_hours_exceeding
FROM job_slot_usage j, percentiles p GROUP BY 1, 2
UNION ALL
SELECT 'P50 Commitment', p.p50_commitment_level, COUNT(*),
  COUNTIF(j.avg_slots_per_job > p.p50_commitment_level),
  ROUND(COUNTIF(j.avg_slots_per_job > p.p50_commitment_level) * 100.0 / COUNT(*), 1),
  ROUND(SUM(CASE WHEN j.avg_slots_per_job > p.p50_commitment_level THEN j.slot_hours ELSE 0 END), 1)
FROM job_slot_usage j, percentiles p GROUP BY 1, 2
ORDER BY commitment_slots
```
